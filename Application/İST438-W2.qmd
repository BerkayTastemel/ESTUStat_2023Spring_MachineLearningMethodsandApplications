---
title: "IST438-W2-Applications"
date:  "Mar 6, 2023"
format: pdf
editor: visual 
---

# Supervised Learning: Linear Regression Models

In this application, we will interest the regression problem under the following sections:

- Training model 
- Measuring model performance
- Checking over and underfitting problem
- Checking model assumptions


# Packages

We need to install `{DALEX}` package to use `apartments` data set in applications. Please use the two-step codes below: (1) install, (2) load the package. 

```{r, warning = FALSE, message = FALSE}
# install.packages("DALEX")
# install.packages("ggplot2")
# install.packages("car")
library(DALEX)
library(ggplot2)
library(car)
```

The first line is `hashtag`ed to faster this process. Do not forget to un-`hashtag` it in your first run. Because any line, which is hashtaged, is not run in R. It is turned the command line! 


# Dataset

We use the `apartments` data set from `{DALEX}` package. It has some features of the apartments in Warsaw, Poland.

```{r}
# calling apartments from {DALEX} package
data(apartments)
```

You can use `str()` function to take a look data set. It returns a data frame consists information about the data set such as:

* number of observation
* number of features (variables)
* name of features
* type of features
* a few observations of features

```{r}
# Take a look to dataset 
str(apartments)
```

# Task: Regression

In this application, we try to train a linear regression model on the `apartments` data set to predict the m^2 price of an intended apartment.


## Step 1 - Splitting the data set

We can use `sample()` function to split the data set as `test` and `train` set. Do not forget to set a seed to reproduce this process in future. It is important to get same observations in each run of these codes.

```{r}
set.seed(123) # for reproducibility
index <- sample(1 : nrow(apartments), round(nrow(apartments) * 0.80))
train <- apartments[index, ]
test  <- apartments[-index, ]
```

In the codes above, we get a sample from a vector has a length is equal to the number of observation in the data set. We must input a sequence as the first argument and the ratio of the train set as the second argument. Then we can save the randomly selected values of the sequence as `index` object. We can set the observations to `train` and `test` objects by using the `index` object.


## Step 2 - Train a linear regression model

We can use the `lm()` function to train a linear regression model. It needs two obligatory arguments: (1) model formula and (2) data set that used to train the model. It is possible to define the model formula like `y ~ x1 + x2 + ...` where `y` is the target variable which is interested features to predict and `x`s are the features that used the information to predict the target variable.

There is a tip about the model formula: you can use `y ~.` instead of defining all features you want to input to the model.

```{r}
lrm_model <- lm(m2.price ~ ., data = train)
```

In above, we train a linear regression model by using the `train` data that we splited in previous step and the model formula. Then, we assigned the output of `lm()` function to the `lrm_model` object. Do not forget that you can give another name to the model object whatever you want, because it is just an object!

Let see the output of the model:

```{r}
lrm_model
```

Model output returns the information about model formula, train data, and the estimated values of model parameters under the `Coefficients` title. You can see the numeric (continuous) features with its name, but 
the categorical features with `its name + name of the category` such as `districtOchota`.

If you want to see more detail about the model, use the `summary()` function.

```{r}
summary(lrm_model)
```

It returns some statistics about the residuals, the model parameters, and also some performance metric values such as the multiple R^2 and the adjusted R^2. These values show the model performance on train data.


## Step 3 - Measuring model performance

It is necessary to check the model performance of the model on test set. Because we are interested to train such a model has a good generalizability performance. To do this, we first calculate the predicted values of the target variable on test set. 

Do not forget to exclude the true values of the target variable from the test set! 

```{r}
predicted_y <- predict(lrm_model, test[,-1])
head(predicted_y)
```

Then, we can calculate some performance metrics of the trained model.

```{r}
error <- test$m2.price - predicted_y
head(error)
```

Mean squared error (MSE), root mean squared error (RMSE), median absolute error (MAE) are the main performance metrics for the model used in regression task.

```{r}
mse_model  <- mean(error ^ 2)  
rmse_model <- sqrt(mean(error ^ 2))
mae_model  <- median(abs(error))
```

Let's compare the values of these metrics:

```{r}
mse_model
rmse_model
mae_model
```

**Q: What are differences between these metrics? Which of these should be used in which situations?**

## Step X1 - Checking the possible over and underfitting problem

The way to check there is any problem related to over or underfitting in the model is to compare the model performance on train and test set. Let's use the RMSE for this:

```{r}
rmse_train <- sqrt(mean((lrm_model$residuals) ^ 2))
rmse_test  <- rmse_model
```

Calculate the difference between th RMSE values:

```{r}
rmse_train - rmse_test
```

The difference is negative means that the performance of the model is better on test set than train set. This may be a sign for overfitting problem because the model performance is better on test set. So, we may say that the model learn more from the train set that cause the poorer performance on test set. But there is no threshold to conclude that there is any problem related to over or underfitting. We can use this difference just as a sign! 


## Step X2 - Checking the bias and variance of the model 

Next week or later!


## Step X3 - Checking the model assumptions

### 1. Linear relationship between features and target variable:

```{r, warning = FALSE, message = FALSE}
p1 <- ggplot(train, aes(surface, m2.price)) + 
  geom_point(size = 1, alpha = .4) +
  geom_smooth(se = FALSE) +
  scale_y_continuous("m2.price") +
  xlab("Surface") 

p2 <- ggplot(train, aes(construction.year, m2.price)) + 
  geom_point(size = 1, alpha = .4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("m2.price") +
  xlab("Construction year")

p3 <- ggplot(train, aes(floor, m2.price)) + 
  geom_point(size = 1, alpha = .4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("m2.price") +
  xlab("Floor")

p4 <- ggplot(train, aes(no.rooms, m2.price)) + 
  geom_point(size = 1, alpha = .4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("m2.price") +
  xlab("Number of rooms")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Solutions for non-linearity:

* Use non-linear models (nonlinear regression or other ML models)
* Transform X and/or Y to obtain a linear relationship using Box-Cox, inverse, or etc.


### 2. Constant variance among residuals (errors)

The errors (residuals) must follows normal distribution with a constant variance. We can check this assumption by using the Shapiro-Wilk test or visual diagnostics.

Let's try to use Shapiro-Wilk test first:

Null hypothesis: errors follow normal the distribution
Alternative hypothesis: errors do not follow the normal distribution

```{r}
shapiro.test(lrm_model$residuals)
```

Because of the p-value is close to $0$, and less than $0.05$, there is enough evidence to reject the null hypothesis. This means that the errors do not follow the normal distribution.

We may see the visual diagnostics to check the constant variance, but in this case it is not necessary because error do not follow normal distribution.

```{r}
con_var <- data.frame(fitted = lrm_model$fitted.values,
                      resid  = lrm_model$residuals)

ggplot(con_var, aes(fitted, resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Predicted values") +
  ylab("Residuals") 
```

In the plot above, the points must be distributed along the line $y = 0$ with a constant pattern in ideal case. 


### 3. No autocorrelation between features (independent variables)

Linear regression assumes the errors are independent and uncorrelated. If in fact, there is correlation among the errors, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be. 

To test the autocorrelation of the residuals, you can use the Durbin-Watson test with the following hypotheses:

Null hypothesis: $\rho_r = 0$
Alternative hypothesis: $\rho_r \neq 0$

```{r}
durbinWatsonTest(lrm_model)
```

Because of the p-value is $0.418 > 0.05$, there is no evidence to reject the null hypothesis. This means that the residuals are not autocorrelated at $95\%$ significance level. 


### 4. More observation than predictors (n > p)

```{r}
# n: number of observations
# p: number of predictors
n <- dim(train)[1] 
p <- dim(train)[2] - 1 # Do not forget to minus the target features because it is not a predictor!

n > p
```


### 5. No multicollinearity between features (independent variables)

**Collinearity** refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the estimation of model parameters, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant. This obviously leads to an inaccurate interpretation of coefficients and makes it difficult to identify influential predictors.

We can use the Variance Inflation Factors (VIFs) to measure the collinearity between features. Let's calculate the VIFs by using the `vif()` function in `{car}` package:

```{r}
vif(lrm_model)
```
If there is any feature has a GVIF value higher than 5 or 10, it may be correlated with the any of other features in the model. There is no consensus about the threshold for multicollinearity, but it is accepted that there is a multicollinearity problem if the VIF/GVIF values are higher than 10 To make the valid this assumption, the easiest solution is to exclude this feature from the model, or you may use more complex regression model instead of linear regression model.

